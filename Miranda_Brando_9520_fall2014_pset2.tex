\documentclass[12pt]{report}
\usepackage{scribe,graphicx,graphics}


\course{MIT 9.520} 	
\coursetitle{Statistical Learning Theory}	
\semester{Fall 2014}
\lecturenumber{2}	
\lecturedate{}		


% Insert your name here!
\scribe{Brando Miranda}

\begin{document}


\maketitle

\paragraph{Problem 1}

\paragraph{Problem 2}
a)

\begin{proof} Let $\mathbf{L} = \mathbf{D} - \mathbf{W}$ as defined in the question. 
To establish equality I will compare the expressions for $R(f) = \frac{1}{2}\sum^m_{i,j=1} W_{ij}(f_i - f_j)^2$ with $R(\mathbf{f}) = \mathbf{f}^T\mathbf{L}\mathbf{f} = \mathbf{f}^T\mathbf{D}\mathbf{f} - \mathbf{f}^T\mathbf{W}\mathbf{f} $ by expanding the square in the first equation and by expanding the matrix multiplications in the second one.

First:
 $$R(f) = \frac{1}{2}\sum^m_{i,j=1} W_{ij}(f_i - f_j)^2 = \frac{1}{2}\sum^m_{i,j=1} (W_{ij}f_i^2 -2W_{ij}f_if_j + W_{ij}f_j^2) = \frac{1}{2}\sum^m_{i,j=1} W_{ij}f_i^2 -\sum^m_{i,j=1}W_{ij}f_if_j + \frac{1}{2}\sum^m_{i,j=1}W_{ij}f_j^2 $$
 
 By symmetry of the weight matrix we get:
 
 $$R(f) = \frac{1}{2}\sum^m_{i,j=1} W_{ij}(f_i - f_j)^2 = \frac{1}{2}\sum^m_{i,j=1} W_{ij}f_i^2 -\sum^m_{i,j=1}W_{ij}f_if_j + \frac{1}{2}\sum^m_{i,j=1}W_{ij}f_j^2 = \sum^m_{i,j=1} W_{ij}f_i^2 -\sum^m_{i,j=1}W_{ij}f_if_j  $$
  
After careful manipulation we can show that 
$\mathbf{f}^T\mathbf{D}\mathbf{f} =  \sum^m_{i,j=1} W_{ij}f_i^2 $ 
and 
$\mathbf{f}^T\mathbf{W}\mathbf{f} = \sum^m_{i,j=1}W_{ij}f_if_j $. This key two key equalities will be shown bellow (which will conclude the proof):
  
$$
\begin{bmatrix}
f_1& \cdots & f_i & \cdots & f_m\\
\end{bmatrix} 
%%
\begin{bmatrix}
\sum_{j=1}^{m}W_{1j} & 0 & \cdots &  & 0 \\
0 & \sum_{j=1}^{m}W_{2j} & 0 & \cdots  & 0 \\
\vdots &   &  &  \ddots  & \vdots \\
0 & \cdots &  & 0, & \sum_{j=1}^{m}W_{mj}
\end{bmatrix} 
%%
\begin{bmatrix}
f_1\\ 
\vdots \\
f_i \\
\vdots \\
f_m\\
\end{bmatrix}
%%
$$

2b)


$$
=
\begin{bmatrix}
f_1& \cdots & f_i & \cdots & f_m\\
\end{bmatrix} 
%
\begin{bmatrix}
f_1 \sum_{j=1}^{m}W_{1j} \\ 
\vdots \\
f_i \sum_{j=1}^{m}W_{ij}\\
\vdots \\
f_m \sum_{j=1}^{m}W_{mj}\\
\end{bmatrix}
=
\sum^m_{i,j=1} W_{ij}f_i^2
$$

%%

$$
\mathbf{f}^T\mathbf{W}\mathbf{f} = 
%%
\begin{bmatrix}
f_1& \cdots & f_i & \cdots & f_m\\
\end{bmatrix} 
%
\begin{bmatrix}
\sum_{j=1}^{m}W_{1j}f_j \\ 
\vdots \\
\sum_{j=1}^{m}W_{ij}f_j\\
\vdots \\
\sum_{j=1}^{m}W_{mj}f_j\\
\end{bmatrix}
=
\sum^m_{i,j=1}W_{ij}f_if_j
%%
$$
Which shows the two equivalence that I required to show the equality between $R(f)$ and $R(\mathbf{f})$
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
b)
Let:
$ \mathbf{f}^T_l = \begin{bmatrix}  f(x_1) & \cdots & f(x_l)  \end{bmatrix} $  ,  $ \mathbf{f}^T_u = \begin{bmatrix}  f(x_{l+1}) & \cdots & f(x_m)  \end{bmatrix}  $, and 
%%%%
$\mathbf{L} = \begin{bmatrix}
\mathbf{L}_{ll}  & \mathbf{L}_{lu} \\
\mathbf{L}_{ul} & \mathbf{L}_{uu}
\end{bmatrix}$

Now express it in those terms:

\begin{equation*}
R(\mathbf{f}) = 
\begin{bmatrix}  \mathbf{f}^T_l  &  \mathbf{f}^T_u  \end{bmatrix} 
%%
\begin{bmatrix}
\mathbf{L}_{ll}  & \mathbf{L}_{lu} \\
\mathbf{L}_{ul} & \mathbf{L}_{uu}
\end{bmatrix}
%%
\begin{bmatrix}
\mathbf{f}_l  \\
\mathbf{f}_u  
\end{bmatrix} \\
\end{equation*}


Plugging in the constraints to the original minimization problem we get:

$$ \underset{\mathbf{f} \in \mathbb{R}^m} {\min} R(\mathbf{f}) = 
\underset{\mathbf{f} \in \mathbb{R}^m} {\min} 
\mathbf{y}^T_l\mathbf{L}_{ll}\mathbf{y}_l
+
\mathbf{y}^T_l\mathbf{L}_{lu}\mathbf{f}_u
+
\mathbf{f}_u\mathbf{L}_{ul}\mathbf{y}_l
+
\mathbf{f}_u\mathbf{L}_{uu}\mathbf{f}_u
=
\underset{\mathbf{f} \in \mathbb{R}^m} {\min}
\mathbf{y}^T_l\mathbf{L}_{lu}\mathbf{f}_u
+
\mathbf{f}_u\mathbf{L}_{ul}\mathbf{y}_l
+
\mathbf{f}_u\mathbf{L}_{uu}\mathbf{f}_u
$$

Now take the gradient wrt to $\mathbf{f}_u$ and set it equal to zero:

$$
\mathbf{L}_{lu}^T\mathbf{y}_l
+
\mathbf{L}_{ul}\mathbf{y}_l
+
2\mathbf{L}_{uu}\mathbf{f}_u = 0
$$


$$
\boxed{
\mathbf{f}^*_{u} = -\frac{1}{2} \mathbf{L}_{uu}^{-1} (\mathbf{L}_{lu}^T+\mathbf{L}_{ul})\mathbf{y}_{l}
}
$$
$$
\boxed{
\mathbf{f}^*_{l} = \mathbf{y}_{l}
}
$$

\paragraph{Problem 3}
Please write your analysis on Problem 3 here


\paragraph{Problem 4}
%%%
Please write your analysis on Problem 4 here
a)


Lemma 1: Let $X_i = I_S[f_i]$, then $\mathbb{E}[X_i] = I[f]$ and by chebyshev's bound therefore:

$$ Pr[ | I_s[f_i] - I[f_i] | \geq \epsilon|] \leq \frac{ Var[X_i] }{ \epsilon^2 } $$

\begin{proof}
Let $X_i = I_S[f_i] = \frac{1}{n} \sum^{n}_{j=1} V(f_i, z_j)$, where $z_j$ is the random samples/data and $f_i$ is a non random (and fixed) function in $\mathcal{H}$. Then, if we take the expectation of $X_i$ wrt to the distribution of $z$ we get:
$$\mathbb{E}[X_i]  = \mathbb{E}[\frac{1}{n} \sum^{n}_{j=1} V(f_i, z_j)] = \frac{1}{n} \sum^{n}_{j=1} \mathbb{E}[V(f_i, z_j)] =  \mathbb{E}[V(f_i, z_j)] = I[f_i]$$

Thus, we can use chebyshev's and thus the following statement is true (concluding proof of lemma):

$$ Pr[ | I_s[f_i] - I[f_i] | \geq \epsilon|] \leq \frac{ Var[X_i] }{ \epsilon^2 } $$

\end{proof}

Theorem: Given the conditions in the question, the the upper bound we are looking for is as follows:

$$Pr[ \sup\limits_{f \in \mathcal{H}}  | I_s[f] - I[f] | \geq \epsilon|] \leq \frac{N(c^2-2cM+M^2)^2}{n \epsilon^2}$$
\begin{proof}

If the largest difference between the empirical risk and the expected risk is larger than $\epsilon$, then that means that the defect (i.e. difference of empirical risk and generalization error) of one of the functions in this finite set is larger than $\epsilon.$ 
i.e. At least one of the defects is larger than $\epsilon$. In equations it reads as follows:

$$Pr[ \sup\limits_{f \in \mathcal{H}}  | I_s[f] - I[f] | \geq \epsilon|] = Pr[\cup^{N}_{i=1} | I_s[f_i] - I[f_i] | \geq \epsilon|] $$

by the union bound:

$$Pr[ \sup\limits_{f \in \mathcal{H}}  | I_s[f] - I[f] | \geq \epsilon|]  \leq \sum^{N}_{i=1} Pr[| I_s[f_i] - I[f_i] | \geq \epsilon|] $$

By Lemma 1 we know:

$$ Pr[ | I_s[f_i] - I[f_i] | \geq \epsilon|] \leq \frac{ Var[X_i] }{ \epsilon^2 } $$

Therefore using this upper bound we can further upper bound our equation above by:

$$ \sum^{N}_{i=1} Pr[| I_s[f_i] - I[f_i] | \geq \epsilon|] \leq \sum^{N}_{i=1} \frac{ Var[I_s[f_i]] }{ \epsilon^2 }$$

Now if we can upper bound the variance, we are done.

$$ Var[I_s[f_i]] = Var[\frac{1}{n} \sum^n_j V(f_i,z_j)]$$

Since the function $f_i$ is fixed (i.e. it was NOT chosen based on the training data) and the only randomness involved is with selecting samples/data from z, then z is the only random variable. However, since its part of the training data and it was sampled in an iid way, then z's are independent. Which makes the cost functions $V(f_i,z_j)$ independent. Therefore, we can use the "linearity" of variance when the random variables are independent to yield the following statement:

$$ Var[\frac{1}{n} \sum^n_j V(f_i,z_j)] = \frac{1}{n^2} \sum^n_j Var[V(f_i,z_j)] = \frac{1}{n}Var[V(f_i,z_j)]  $$

Now lets use the definition variance to upper bound the above:

$$Var[V(f_i,z_j)] = \mathbb{E}[V(f_i,z_j)^2] - \mathbb{E}[V(f_i,z_j)]^2$$

To upper bound the above we need to upper bound $\mathbb{E}[V(f_i,z_j)^2]$ and lower bound $\mathbb{E}[V(f_i,z_j)]^2$. 

It is easy to lower bound $\mathbb{E}[V(f_i,z_j)]^2$ because according to the question, it is the squared loss function, which can never be less than zero. Therefore, the lower bound is:

$$\mathbb{E}[V(f_i,z_j)]^2 \geq 0$$

To upper bound  $\mathbb{E}[V(f_i,z_j)]^2$ we need to substitute in the definition of the squared loss function:

$$\mathbb{E}[V(f_i,z_j)^2] = \mathbb{E}[f_i(x)^2 - 2f_i(x)y + y^2] $$

Since $sup_x \in X |f(x)| \leq C$ and the max value of any y is $M$, then we have:


$$\mathbb{E}[V(f_i,z_j)^2] \leq \mathbb{E}[(C^2 - 2CM + M^2)^2] = (C^2 - 2CM + M^2)^2 $$

Setting the terms we desired to be their maximum value and minimum value respectively, we get:

$$Var[V(f_i,z_j)] = \mathbb{E}[V(f_i,z_j)^2] - \mathbb{E}[V(f_i,z_j)]^2 \leq (C^2 - 2CM + M^2)^2 - 0 = (C^2 - 2CM + M^2)^2 $$

$$Var[V(f_i,z_j)] \leq (C^2 - 2CM + M^2)^2$$

Yielding: 

$$Var[I_s[f_i]] \leq \frac{1}{n} (C^2 - 2CM + M^2)^2$$

However, our goal is to bound the following:

$$Pr[ \sup\limits_{f \in \mathcal{H}}  | I_s[f] - I[f] | \geq \epsilon|] \leq \sum^{N}_{i=1} \frac{ Var[I_s[f_i]] }{ \epsilon^2 }$$

Now that we have an upper bound on the variance of the empirical risk, lets upper bound the desired probability:

$$Pr[ \sup\limits_{f \in \mathcal{H}}  | I_s[f] - I[f] | \geq \epsilon|] \leq \sum^{N}_{i=1} \frac{1}{n} \frac{ (C^2 - 2CM + M^2)^2 }{ \epsilon^2 } = \frac{N}{n} \frac{ (C^2 - 2CM + M^2)^2 }{ \epsilon^2 }$$

Yielding the desired upper bound.
\end{proof}
%%%

b)

%%%

\begin{proof}
Let $f_s = argmin_{f \in \mathcal{H}} I_s[f]$, i.e. the minimizer of the empirical risk. Then obviously the defect wrt to this the minimizer of the empirical risk is upper bounded by the probability from the previous part of the question, i.e.

$$Pr[| I_S[f_s] - I[f_s] | \geq \epsilon|]= Pr[ \sup\limits_{f \in \mathcal{H}}  | I_s[f] - I[f] | \geq \epsilon|] \leq \sum^{N}_{i=1} Pr[| I_s[f_i] - I[f_i] | \geq \epsilon|] \leq \frac{N(c^2-2cM+M^2)^2}{n \epsilon^2}$$


The difference between generalization and empirical error is either above epsilon or bellow it with probability 1. Therefore:

$$1 - Pr[| I_S[f_s] - I[f_s] | \leq \epsilon|] = Pr[| I_S[f_s] - I[f_s] | \geq \epsilon|] $$

So we have after some algebra we have:

$$Pr[| I_S[f_s] - I[f_s] | \leq \epsilon |] \geq 1- \frac{N(c^2-2cM+M^2)^2}{n \epsilon^2} $$

If we want to have at least $1 - \eta$ confidence that the empirical error will be $\epsilon-$close to the generalization error the equation bellow must hold:

$$Pr[| I_S[f_s] - I[f_s] | \leq \epsilon |] \geq 1- \frac{N(c^2-2cM+M^2)^2}{n \epsilon^2} \geq 1- \eta$$

Therefore, the closeness of empirical and generalization error holds (i.e. the above inequality holds) holds if the following holds:

$$1- \frac{N(c^2-2cM+M^2)^2}{n \epsilon^2} \geq 1- \eta$$

Which after some algebra implies:

$$\epsilon \geq \sqrt{\frac{N(c^2-2cM+M^2)^2}{n \epsilon^2}}$$

Meaning that if we want $1 - \eta$ confidence, then the smallest error bound we can bound with probability, is $\sqrt{\frac{N(c^2-2cM+M^2)^2}{n \epsilon^2}}$

Therefore, that means that the difference between the true generalization error and the empirical risk is at most $\sqrt{\frac{N(c^2-2cM+M^2)^2}{n \epsilon^2}}$ with $1 -\eta$ confidence:

$$| I_S[f_s] - I[f_s] | \leq \epsilon | \geq \epsilon = \epsilon(n, \eta, N) =   \sqrt{ \frac{N(c^2-2cM+M^2)^2}{n \epsilon^2} } $$

With the the absolute value function we have $I_s[f_s] - I[f_s] \leq e(n, \eta, N)$ and $I[f_s] - I_s[f_s] \leq e(n, \eta, N) $ are true. Since we are interested in bounding the generalization error we will be interested in the second inequality (the first one is true but useless because it bounds the empirical error in terms of the generalization error. We don't need to bound the empirical error because we can compute it directly!) Thus:

$$I[f_s] - I_s[f_s] \leq \epsilon(n, \eta, N) $$

$$I[f_s] \leq I_s[f_s] + \epsilon(n, \eta, N) =  I_s[f_s] + \sqrt{ \frac{N(c^2-2cM+M^2)^2}{n \epsilon^2} }$$

\end{proof}

As $N = |\mathcal{H}|$ increases, then that means the we are increasing the space of functions we are allowing ourselves to choose from. Since we are adding more functions to choose from without removing previous functions we already had in $\mathcal{H}$, the empirical risk cannot increase. If this is true, then the empirical risk can only decrease. This means that $I_S[f_S]$ can only decrease as $N$ increases. So we might get a lower empirical risk. 

However, as N increases, $\epsilon(n, \eta, N)$ increases wrt to the growth of  the square root of N . Meaning that our difference between empirical and generalization error becomes more lose, in the sense that the upper bound increases. This is not good because if N becomes very large, then the event that we are trying to impose a probability, will become less and less interesting. To illustrate my argument, take as an extreme case, if $\epsilon(n, \eta, N) = \infty$, then we are trying to impose an upper bound on the probability that the empirical risk is really far from the generalization. However, if we allow the difference to be infinity, then it becomes an uninteresting probabilistic event, even if we know the probability exactly. i.e. we know for sure that their difference will be less than infinity if both are finite real numbers.

Now lets explore the sum of them $I_S[f_S] + \epsilon(n, \eta, N)$. As N increases $I_S[f_S]$ can only decrease and $\epsilon(n, \eta, N)$ increases for sure. However, $I_S[f_S]$ doesn't necessarily have to decrease for every unit of N that we increase, while $\epsilon(n, \eta, N)$ for sure increases for every unit of N. This makes overfitting more likely. As N increases $I_S[f_S]$ \textbf{might} decrease, however, we know for sure that $\epsilon(n, \eta, N)$ increases. This gives us an upper bound on $I[f_S]$ (generalization error) that increases most of the times as $N$ increases. This means that our upper bound estimate for the generalization error becomes more and more useless and that the empirical error becomes less good as a proxy for $I[f_S]$. Thus, yielding overfitting more and more likely.

4c)

\begin{proof}

If we want with $1 - \eta$ confidence that the generalization and empirical error are close, then from ideas explained in part b the following equation holds:

$$ | I[f_s] - I_s[f_s] | \leq \epsilon(n, \eta, N)$$

and

$$ | I[f^*] - I_s[f^*] | \leq \epsilon(n, \eta, N)$$

Therefore we know that:

$$ I[f_s] - I_s[f_s] \leq \epsilon(n, \eta, N)$$

and

$$ I_S[f^*] - I[f^*] \leq \epsilon(n, \eta, N)$$

Add the above two inequalities:

$$I[f_s] - I_s[f_s] +  I_s[f^*] - I[f^*] = (I[f_s] - I[f^*]) + (I_S[f^*] - I_s[f_s])  \leq 2\epsilon(n, \eta, N)$$

The second expression in brackets is greater than or equal to zero i.e. $I_s[f^*] - I_s[f_s] \geq 0$. This is because $f_s$ is chosen such that it minimizes the empirical risk. Therefore, there cannot be any function that achieves a lower empirical risk that the minimizer of the empirical risk. Therefore:

 $$I_S[f^*] \geq I_s[f_s] \implies I_S[f^*] - I_s[f_s] \geq 0$$
 
 Therefore we know that $I[f_s] - I_s[f_s] \leq (I[f_s] - I[f^*]) + (I_s[f^*] - I_s[f_s]) $. Therefore implying that 
 
 $$I[f_s] - I[f^*] \leq (I[f_s] - I[f^*]) + (I_S[f^*] - I_s[f_s])  \leq 2\epsilon(n, \eta, N)$$
 
  $$I[f_s] - I[f^*] \leq 2\epsilon(n, \eta, N)$$
 
 As required.

\end{proof}

%%%
\paragraph{Problem 5 (MATLAB)}
Please write your analysis on Problem 5 here

\end{document}

